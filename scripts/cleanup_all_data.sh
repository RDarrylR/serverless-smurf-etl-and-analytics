#!/bin/bash
#
# Cleanup All Data Script
# Removes all data from S3 and DynamoDB generated by the ETL pipeline
#
# This script will delete:
# - S3: uploads/, processed/, quicksight/, rejected/ prefixes
# - DynamoDB: All items in the SalesData table
#
# Usage: ./cleanup_all_data.sh [--yes]
#   --yes    Skip confirmation prompts (for automation)
#

set -e

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
INFRA_DIR="$PROJECT_DIR/infrastructure"

# Get S3 bucket from terraform output or environment variable
if [ -n "$S3_BUCKET" ]; then
    : # Use environment variable if set
elif [ -d "$INFRA_DIR/.terraform" ]; then
    S3_BUCKET=$(cd "$INFRA_DIR" && terraform output -raw s3_bucket_name 2>/dev/null) || true
fi

if [ -z "$S3_BUCKET" ]; then
    echo "Error: S3_BUCKET not set. Either:"
    echo "  1. Set S3_BUCKET environment variable: export S3_BUCKET=your-bucket-name"
    echo "  2. Ensure terraform has been applied in infrastructure/"
    exit 1
fi

DYNAMODB_TABLE="SalesData"

# Parse arguments
SKIP_CONFIRM=false
while [[ $# -gt 0 ]]; do
    case $1 in
        --yes|-y)
            SKIP_CONFIRM=true
            shift
            ;;
        --help)
            echo "Usage: $0 [--yes]"
            echo "  --yes    Skip confirmation prompts"
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

echo "=============================================="
echo "Data Cleanup Script"
echo "=============================================="
echo ""
echo "This will DELETE all data from:"
echo "  - S3 bucket: s3://$S3_BUCKET/"
echo "    - uploads/*"
echo "    - processed/*"
echo "    - quicksight/*"
echo "    - rejected/*"
echo "  - DynamoDB table: $DYNAMODB_TABLE"
echo ""
echo "=============================================="
echo ""

# Show current data counts
echo "Current data in S3:"
for prefix in uploads processed quicksight rejected; do
    COUNT=$(aws s3 ls "s3://$S3_BUCKET/$prefix/" --recursive 2>/dev/null | wc -l | tr -d ' ')
    echo "  $prefix/: $COUNT files"
done
echo ""

echo "Current data in DynamoDB:"
DYNAMO_COUNT=$(aws dynamodb scan --table-name "$DYNAMODB_TABLE" --select "COUNT" --query 'Count' --output text 2>/dev/null || echo "0")
echo "  $DYNAMODB_TABLE: $DYNAMO_COUNT items"
echo ""

# Confirm before deleting
if [ "$SKIP_CONFIRM" = false ]; then
    echo "WARNING: This action cannot be undone!"
    echo ""
    read -p "Are you sure you want to delete ALL data? (type 'DELETE' to confirm): " CONFIRM
    if [ "$CONFIRM" != "DELETE" ]; then
        echo "Cleanup cancelled."
        exit 0
    fi
    echo ""
fi

echo "Starting cleanup..."
echo ""

# Delete S3 data
echo "Deleting S3 data..."
for prefix in uploads processed quicksight rejected; do
    echo "  Deleting s3://$S3_BUCKET/$prefix/..."
    aws s3 rm "s3://$S3_BUCKET/$prefix/" --recursive --quiet 2>/dev/null || true
    echo "    Done"
done
echo ""

# Delete DynamoDB data
echo "Deleting DynamoDB data from $DYNAMODB_TABLE..."

# Get table key schema
KEY_SCHEMA=$(aws dynamodb describe-table --table-name "$DYNAMODB_TABLE" --query 'Table.KeySchema' --output json)
HASH_KEY=$(echo "$KEY_SCHEMA" | python3 -c "import json,sys; keys=json.load(sys.stdin); print([k['AttributeName'] for k in keys if k['KeyType']=='HASH'][0])")
RANGE_KEY=$(echo "$KEY_SCHEMA" | python3 -c "import json,sys; keys=json.load(sys.stdin); rk=[k['AttributeName'] for k in keys if k['KeyType']=='RANGE']; print(rk[0] if rk else '')" 2>/dev/null || echo "")

echo "  Table key: $HASH_KEY" $([ -n "$RANGE_KEY" ] && echo "(range: $RANGE_KEY)")

# Scan and delete in batches
DELETED_COUNT=0
while true; do
    # Scan for items (limit to 25 for batch delete)
    if [ -n "$RANGE_KEY" ]; then
        ITEMS=$(aws dynamodb scan \
            --table-name "$DYNAMODB_TABLE" \
            --projection-expression "#pk, #sk" \
            --expression-attribute-names "{\"#pk\":\"$HASH_KEY\",\"#sk\":\"$RANGE_KEY\"}" \
            --limit 25 \
            --output json)
    else
        ITEMS=$(aws dynamodb scan \
            --table-name "$DYNAMODB_TABLE" \
            --projection-expression "#pk" \
            --expression-attribute-names "{\"#pk\":\"$HASH_KEY\"}" \
            --limit 25 \
            --output json)
    fi

    COUNT=$(echo "$ITEMS" | python3 -c "import json,sys; print(len(json.load(sys.stdin).get('Items',[])))")

    if [ "$COUNT" -eq 0 ]; then
        break
    fi

    # Build batch delete request
    DELETE_REQUESTS=$(echo "$ITEMS" | python3 -c "
import json
import sys

data = json.load(sys.stdin)
items = data.get('Items', [])
requests = []

for item in items:
    key = {}
    for k, v in item.items():
        key[k] = v
    requests.append({'DeleteRequest': {'Key': key}})

print(json.dumps({'$DYNAMODB_TABLE': requests}))
")

    # Execute batch delete
    aws dynamodb batch-write-item --request-items "$DELETE_REQUESTS" --output text > /dev/null

    DELETED_COUNT=$((DELETED_COUNT + COUNT))
    echo "  Deleted $DELETED_COUNT items..."
done

echo "  Total deleted: $DELETED_COUNT items"
echo ""

echo "=============================================="
echo "Cleanup Complete"
echo "=============================================="
echo ""
echo "All data has been removed. Summary:"
echo "  - S3: All prefixes cleared"
echo "  - DynamoDB: $DELETED_COUNT items deleted"
echo ""
echo "You can now upload fresh data using upload_sample_data.sh"
echo "=============================================="
